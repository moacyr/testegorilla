{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moacyr/testegorilla/blob/main/Gorilla_hosted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gorilla Hosted - Try it out in less than 60s üöÄ\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ShishirPatil/gorilla)  [![arXiv](https://img.shields.io/badge/arXiv-2305.15334-<COLOR>.svg?style=flat-square)](https://arxiv.org/abs/2305.15334)   [![Discord](https://img.shields.io/discord/1111172801899012102?label=Discord&logo=discord&logoColor=green&style=flat-square)](https://discord.gg/SwTyuTAxX3)  [![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/shishirpatil_/status/1661780076277678082)](https://twitter.com/shishirpatil_/status/1661780076277678082)\n",
        "\n",
        "Play around with Gorilla! Here, we host the Gorilla zero-shot models, so you can try it out! This is compatible with the OpenAI chat completion API - plug and play!\n",
        "\n",
        "üü¢ Now with Apache-2.0! Gorilla is commercially usable with no obligations üöÄ\n",
        "\n",
        "We are happy to launch all three models: `gorilla-7b-hf-v1` which chooses from 925 Hugging Face APIs 0-shot, `gorilla-7b-th-v0` for 94 (exhaustive) Tensor Hub APIs 0-shot, `gorilla-7b-tf-v0` for 626 (exhaustive) Tensorflow Hub APIs 0-shot. `gorilla-mpt-7b-hf-v0` and `gorilla-falcon-7b-hf-v0`are two Apache-2.0 licensed models for Hugging Face APIs. We have a hosted end-point for `gorilla-mpt-7b-hf-v0` in this colab, and are in the process of adding `gorilla-falcon-7b-hf-v0` soon! In spirit of openess, we do not filter, nor carry out any post processing either to the prompt nor response. We will release the combined {HF+TF+TH} model which also has generic chat capability slowly to accomodate server demand.\n",
        "\n",
        "üíÉ If you want to use Gorilla or build on top of it! Feel absolutely free to do so - we believe in open source research and you don't even have to tell us! In case you choose to do, we have a vibrant community in Discord! Stop by and say Hi üëã\n",
        "\n",
        "<img src=\"https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png?raw=true\" width=30% height=30%>"
      ],
      "metadata": {
        "id": "7bKku43frr8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gorilla ü¶ç is hosted by UC Berkeley Sky lab for FREE ü§© as a research prototype ü§ì\n",
        "## Please don't use it for commercial serving üëÄ\n",
        "## The hosted models are only trained to serve HuggingFace/TF/Torch APIs. They are NOT trained to serve other restful APIs."
      ],
      "metadata": {
        "id": "5PA9GQbV4rcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eBd_fso7qFPX"
      },
      "outputs": [],
      "source": [
        "# Import Chat completion template and set-up variables\n",
        "!pip install openai &> /dev/null\n",
        "import openai\n",
        "import urllib.parse\n",
        "\n",
        "openai.api_key = \"EMPTY\" # Key is ignored and does not matter\n",
        "openai.api_base = \"http://zanino.millennium.berkeley.edu:8000/v1\"\n",
        "# Alternate mirrors\n",
        "# openai.api_base = \"http://34.132.127.197:8000/v1\"\n",
        "\n",
        "# Report issues\n",
        "def raise_issue(e, model, prompt):\n",
        "    issue_title = urllib.parse.quote(\"[bug] Hosted Gorilla: <Issue>\")\n",
        "    issue_body = urllib.parse.quote(f\"Exception: {e}\\nFailed model: {model}, for prompt: {prompt}\")\n",
        "    issue_url = f\"https://github.com/ShishirPatil/gorilla/issues/new?assignees=&labels=hosted-gorilla&projects=&template=hosted-gorilla-.md&title={issue_title}&body={issue_body}\"\n",
        "    print(f\"An exception has occurred: {e} \\nPlease raise an issue here: {issue_url}\")\n",
        "\n",
        "# Query Gorilla server\n",
        "def get_gorilla_response(prompt=\"I would like to translate from English to French.\", model=\"gorilla-7b-hf-v1\"):\n",
        "  try:\n",
        "    completion = openai.ChatCompletion.create(\n",
        "      model=model,\n",
        "      messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "  except Exception as e:\n",
        "    raise_issue(e, model, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßë‚Äçüíª [Update Jun 15] With our new v1 model `gorilla-7b-hf-delta-v1`, Gorilla now returns code snippets you can use directly in your workflow!"
      ],
      "metadata": {
        "id": "FNDBCAMBV0aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Translation ‚úç with ü§ó"
      ],
      "metadata": {
        "id": "asdPNq38qIx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla `gorilla-mpt-7b-hf-v1` with code snippets\n",
        "# Translation\n",
        "prompt = \"I would like to translate 'I feel very good today.' from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2lVWV9yWO0i",
        "outputId": "ad0b4c9e-557f-4760-b2f4-8b6d4eb65709"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Natural Language Processing Translation\n",
            "<<<api_call>>>: translation = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the pipeline function from the transformers library provided by Hugging Face.\n",
            "2. Initialize a translation pipeline using the 'translation_en_to_zh' task with the pretrained 'Helsinki-NLP/opus-mt-en-zh' model, which is designed for translating English text to Chinese.\n",
            "3. Use the translation pipeline to translate the given English text to Chinese.<<<code>>>:\n",
            "from transformers import pipeline\n",
            "\n",
            "def load_model():\n",
            "    translation = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\n",
            "    return translation\n",
            "\n",
            "def process_data(text, translation):\n",
            "    response = translation(text, max_length=100)[0]['translation_text']\n",
            "    return response\n",
            "\n",
            "# Input text\n",
            "text = 'I feel very good today.'\n",
            "\n",
            "# Load the translation model\n",
            "translation = load_model()\n",
            "\n",
            "# Process the data\n",
            "response = process_data(text, translation)\n",
            "\n",
            "# Print translated text\n",
            "print(response)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: Object detection üî∑ with ü§ó"
      ],
      "metadata": {
        "id": "Gnx7YHf18DTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla `gorilla-7b-hf-v1` with code snippets\n",
        "# Object Detection\n",
        "prompt = \"I want to build a robot that can detecting objects in an image ‚Äòcat.jpeg‚Äô. Input: [‚Äòcat.jpeg‚Äô]\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvQawbSxWoEu",
        "outputId": "cf8e992e-8a44-4561-88d5-12b9993b14b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Computer Vision Object Detection\n",
            "<<<api_call>>>: model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the necessary components from the Hugging Face Transformers library, torch, and PIL (Python Imaging Library).\n",
            "2. Open the image using PIL's Image.open() function with the provided image path.\n",
            "3. Initialize the pretrained DETR (DEtection TRansformer) model and the image processor.\n",
            "4. Generate inputs for the model using the image processor.\n",
            "5. Pass the inputs to the model, which returns object detection results.\n",
            "<<<code>>>:\n",
            "\n",
            "from transformers import AutoFeatureExtractor, AutoModelForObjectDetection\n",
            "from PIL import Image\n",
            "import torch\n",
            "\n",
            "def load_model():\n",
            "    feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\n",
            "    model = AutoModelForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\n",
            "    return feature_extractor, model\n",
            "\n",
            "def process_data(image_path, feature_extractor, model):\n",
            "    image = Image.open(image_path)\n",
            "    inputs = feature_extractor(images=image, return_tensors='pt')\n",
            "    outputs = model(**inputs)\n",
            "    results = feature_extractor.post_process(outputs, threshold=0.6)[0]\n",
            "    response = [model.config.id2label[label.item()] for label in results['labels']]\n",
            "    return response\n",
            "\n",
            "image_path = 'cat.jpeg'\n",
            "\n",
            "# Load the model and feature extractor\n",
            "feature_extractor, model = load_model()\n",
            "\n",
            "# Process the data\n",
            "response = process_data(image_path, feature_extractor, model)\n",
            "print(response)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try to invoke APIs from Torch Hub instead for the same prompts!"
      ],
      "metadata": {
        "id": "Ot4EKOLXhpoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation ‚úç with Torch Hub\n",
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-th-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS-ZeP0rhmzp",
        "outputId": "2c89daf3-6a27-4b29-8371-483e6925dca7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'domain': 'Neural Machine Translation', 'api_call': \\\"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_mt_cnn', pretrained=True)\\\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained NVIDIA Machine Translation model with a CNN architecture for English to Chinese translation.', 'code': 'import torch\\ntarget_language = 'zh'\\ndevice = torch.device('cpu')\\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_mt_cnn', pretrained=True)'}\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚õ≥Ô∏è With Gorilla being fine-tuned on MPT, and Falcon, you can use Gorilla commercially with no obligations! üü¢"
      ],
      "metadata": {
        "id": "hvQ3q5AX1wqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla with `gorilla-mpt-7b-hf-v0`\n",
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-mpt-7b-hf-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGi3wwmQ1voP",
        "outputId": "99d7af83-c705-495c-e7b1-445ae5e6c4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide the English text you would like to translate: \\\"Translate this text to Chinese:\\\"\\n<<<domain>>>: Natural Language Processing Text2Text Generation\\n<<<api_call>>>: M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. Import the necessary libraries - M2M100ForConditionalGeneration and M2M100Tokenizer from the transformers library.\\n2. Load the pretrained model 'facebook/m2m100_1.2B' and its corresponding tokenizer.\\n3. Set the source language to English (en) and use the tokenizer to tokenize the input text.\\n4. Use the model to generate the translated text in Chinese by providing the tokenized input to the 'generate' function.\\n5. Decode the generated tokens back into a readable text string using the tokenizer.\\n<<<code>>>: from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\nsrc_text = \\\"Translate this text to Chinese:\\\"\\nsrc_lang = \\\"en\\\"\\nmodel = M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\ntokenizer = M2M100Tokenizer.from_pretrained('facebook/m2m100_1.2B')\\ntokenized_input = tokenizer(src_text, return_tensors=\\\"pt\\\", src_lang=src_lang)\\ntranslated_tokens = model.generate(**tokenized_input)\\ntranslated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\\n\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We will deprecate the `gorilla-7b-hf-v0` model on July 4 when we will automatically upgrade all v0 model requests to v1. The only changes between v0 and v1 is better code snippets.\n",
        "Below are example prompt-responses for `gorilla-7b-hf-v0` Legacy Model for ü§ó"
      ],
      "metadata": {
        "id": "fIRsh6Ne9f0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v0\" ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnJzPaN5FlV",
        "outputId": "732abf5c-5c54-498c-c8ba-22d1d7161539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Natural Language Processing Text2Text Generation\n",
            "<<<api_call>>>: M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the necessary libraries, which are M2M100ForConditionalGeneration and M2M100Tokenizer from the transformers package.\n",
            "2. Load the pre-trained model 'facebook/m2m100_1.2B' using the M2M100ForConditionalGeneration.from_pretrained() method. This model is designed for machine-to-machine translation tasks.\n",
            "3. Load the tokenizer using the M2M100Tokenizer.from_pretrained() method. This tokenizer is used to prepare the input text for the model and convert the translated output back into human-readable text.\n",
            "4. Define the source text for translation and tokenize it using the tokenizer.\n",
            "5. Use the model to generate the translated text using the source tokens as input.\n",
            "6. Decode the translated text using the tokenizer and print the result.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I want to build a robot that can detect objects in an image.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WochUPqf8HLa",
        "outputId": "a25c281f-d3ff-4fcd-d4ac-ff375f613484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Computer Vision Object Detection\n",
            "<<<api_call>>>: YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. We first import the necessary classes from the transformers, PIL, and requests packages. This includes YolosForObjectDetection for the object detection model and Image for processing image data.\n",
            "2. We then use the from_pretrained method of the YolosForObjectDetection class to load the pre-trained model 'hustvl/yolos-tiny'. This model has been trained for object detection tasks, which is exactly what we need for detecting objects in an image.\n",
            "3. We load the image data from a file or a URL, and then use the model to analyze the image and identify the objects within it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I want return a list of tools and machines are in a photo.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naDZkmdXyQ6_",
        "outputId": "191c0638-ba3b-45fd-bbdd-28c8263b6435"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Computer Vision Object Detection\n",
            "<<<api_call>>>: model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the necessary components from the transformers library and Python Image Library (PIL).\n",
            "2. Load the YolosForObjectDetection model from Hugging Face with the pretrained weights of 'hustvl/yolos-tiny'.\n",
            "3. Create an image processor that can process the input images and return the appropriate tensors.\n",
            "4. Process the input image using the image processor and extract the features.\n",
            "5. Pass the features to the model to receive the predictions for objects in the image.\n",
            "6. Filter the predictions based on the object labels and print the detected objects along with their bounding boxes.\n",
            "<<<code>>>:\n",
            "from transformers import AutoFeatureExtractor, AutoModelForObjectDetection\n",
            "import torch\n",
            "from PIL import Image\n",
            "\n",
            "def load_model():\n",
            "    feature_extractor = AutoFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\n",
            "    model = AutoModelForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
            "    return feature_extractor, model\n",
            "\n",
            "def process_data(image_path, feature_extractor, model):\n",
            "    image = Image.open(image_path)\n",
            "    inputs = feature_extractor(images=image, return_tensors='pt')\n",
            "    outputs = model(**inputs)\n",
            "    target_sizes = torch.tensor([image.size[::-1]])\n",
            "    results = feature_extractor.post_process(outputs=outputs, target_sizes=target_sizes)\n",
            "    boxes, scores, labels = results[0]['boxes'], results[0]['scores'], results[0]['labels']\n",
            "    response = [{'box': box, 'score': score, 'label': feature_extractor.target_mapping[label]} for box, score, label in zip(boxes, scores, labels)]\n",
            "    return response\n",
            "\n",
            "image_path = 'car.jpg'\n",
            "\n",
            "# Load\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Using gorilla is as easy as calling `get_gorilla_response()` with your prompt! Try out Gorilla, and share your interesting findings in `#showcase` ü§© [Discord](https://discord.gg/3apqwwME)!"
      ],
      "metadata": {
        "id": "XS5Qe6zD8tdX"
      }
    }
  ]
}